{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydxzht-q7Vp_"
   },
   "source": [
    "# Home 4: Build a CNN for image recognition.\n",
    "\n",
    "### Name: Harman Singh Bath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJYl51vS7VqC"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05v6Sykj7VqD"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TaAXc42_7VqE"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "3h9BS4m_7VqF",
    "outputId": "df218bef-85d3-4ccf-a3cf-53f24d1dc867"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YG1LLt8q7VqI"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. \n",
    "One-hot encode transform such a scalar to a $10$-dim vector.\n",
    "\n",
    "E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Ifj3g8u37VqJ",
    "outputId": "0882c504-ecb9-4a94-ba03-7727e36a6b30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    one_hot = numpy.zeros((y.shape[0], 10))\n",
    "    one_hot[numpy.arange((y.shape[0])),y[:,0]] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJN4Q-gu7VqM"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTluGin57VqM"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "b304qFai7VqN",
    "outputId": "7fa5a083-51f1-4327-9594-3a65f851ba09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yw1RJRpu7VqR"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7ch_EZT7VqS"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "IHes4HtP7VqS",
    "outputId": "e06f1b53-eb0b-4709-ed84-457a66a979ab",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 552,874\n",
      "Trainable params: 551,722\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48ZmxWtWeAYB"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# learning rate schedule\n",
    "class step_decay():\n",
    "    def __init__(self):\n",
    "        self.initial_lrate = 0.0005\n",
    "        self.drop = 0.5\n",
    "        self.epochs_drop = 40.0\n",
    "        \n",
    "    def __call__(self, epoch):\n",
    "        lrate = self.initial_lrate * numpy.power(self.drop, numpy.floor((1+epoch)/self.epochs_drop))\n",
    "        print(\"lrate: \", lrate)\n",
    "        return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Dxk81JMXHlt"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.0),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay())\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uR28jh3b5Bv"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,  \n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True,\n",
    "        data_format='channels_last',\n",
    "        validation_split=0.0)\n",
    "\n",
    "datagen.fit(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "92d0sQtUahyK",
    "outputId": "24813f97-4b84-45b9-8023-becf42003141",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.9204 - acc: 0.6851 - val_loss: 0.8297 - val_acc: 0.7320\n",
      "Epoch 2/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.9139 - acc: 0.6903 - val_loss: 0.7686 - val_acc: 0.7496\n",
      "Epoch 3/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.9074 - acc: 0.6904 - val_loss: 0.7328 - val_acc: 0.7596\n",
      "Epoch 4/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.8920 - acc: 0.6960 - val_loss: 0.7691 - val_acc: 0.7484\n",
      "Epoch 5/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.8833 - acc: 0.6987 - val_loss: 0.7523 - val_acc: 0.7506\n",
      "Epoch 6/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.8803 - acc: 0.7026 - val_loss: 0.8059 - val_acc: 0.7403\n",
      "Epoch 7/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.8754 - acc: 0.7046 - val_loss: 0.9147 - val_acc: 0.7123\n",
      "Epoch 8/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8691 - acc: 0.7056 - val_loss: 0.7019 - val_acc: 0.7722\n",
      "Epoch 9/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8586 - acc: 0.7085 - val_loss: 0.7402 - val_acc: 0.7610\n",
      "Epoch 10/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.8569 - acc: 0.7105 - val_loss: 0.7885 - val_acc: 0.7563\n",
      "Epoch 11/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8445 - acc: 0.7129 - val_loss: 0.7060 - val_acc: 0.7685\n",
      "Epoch 12/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8432 - acc: 0.7127 - val_loss: 0.6752 - val_acc: 0.7753\n",
      "Epoch 13/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8328 - acc: 0.7187 - val_loss: 0.7179 - val_acc: 0.7691\n",
      "Epoch 14/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8263 - acc: 0.7185 - val_loss: 0.6925 - val_acc: 0.7802\n",
      "Epoch 15/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8261 - acc: 0.7221 - val_loss: 0.6365 - val_acc: 0.7933\n",
      "Epoch 16/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.8194 - acc: 0.7258 - val_loss: 0.6829 - val_acc: 0.7766\n",
      "Epoch 17/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8091 - acc: 0.7256 - val_loss: 0.6062 - val_acc: 0.7975\n",
      "Epoch 18/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8079 - acc: 0.7248 - val_loss: 0.7209 - val_acc: 0.7756\n",
      "Epoch 19/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8020 - acc: 0.7277 - val_loss: 0.6348 - val_acc: 0.7922\n",
      "Epoch 20/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8031 - acc: 0.7286 - val_loss: 0.6853 - val_acc: 0.7790\n",
      "Epoch 21/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7944 - acc: 0.7288 - val_loss: 0.6443 - val_acc: 0.7869\n",
      "Epoch 22/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7914 - acc: 0.7309 - val_loss: 0.7568 - val_acc: 0.7640\n",
      "Epoch 23/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7827 - acc: 0.7346 - val_loss: 0.6881 - val_acc: 0.7791\n",
      "Epoch 24/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.7842 - acc: 0.7369 - val_loss: 0.6705 - val_acc: 0.7823\n",
      "Epoch 25/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7785 - acc: 0.7383 - val_loss: 0.5810 - val_acc: 0.8060\n",
      "Epoch 26/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7770 - acc: 0.7395 - val_loss: 0.6366 - val_acc: 0.7923\n",
      "Epoch 27/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7689 - acc: 0.7424 - val_loss: 0.6233 - val_acc: 0.7979\n",
      "Epoch 28/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7669 - acc: 0.7425 - val_loss: 0.6036 - val_acc: 0.8022\n",
      "Epoch 29/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.7715 - acc: 0.7404 - val_loss: 0.6504 - val_acc: 0.7944\n",
      "Epoch 30/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7659 - acc: 0.7430 - val_loss: 0.5928 - val_acc: 0.8098\n",
      "Epoch 31/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.7627 - acc: 0.7421 - val_loss: 0.6945 - val_acc: 0.7897\n",
      "Epoch 32/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7466 - acc: 0.7477 - val_loss: 0.6121 - val_acc: 0.8000\n",
      "Epoch 33/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.7548 - acc: 0.7461 - val_loss: 0.5983 - val_acc: 0.8082\n",
      "Epoch 34/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.7486 - acc: 0.7470 - val_loss: 0.6098 - val_acc: 0.8072\n",
      "Epoch 35/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7492 - acc: 0.7495 - val_loss: 0.6581 - val_acc: 0.7898\n",
      "Epoch 36/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7513 - acc: 0.7478 - val_loss: 0.5381 - val_acc: 0.8236\n",
      "Epoch 37/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7499 - acc: 0.7482 - val_loss: 0.6185 - val_acc: 0.8016\n",
      "Epoch 38/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7378 - acc: 0.7516 - val_loss: 0.5814 - val_acc: 0.8085\n",
      "Epoch 39/500\n",
      "lrate:  0.0005\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7417 - acc: 0.7487 - val_loss: 0.6378 - val_acc: 0.7996\n",
      "Epoch 40/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.7172 - acc: 0.7585 - val_loss: 0.5774 - val_acc: 0.8154\n",
      "Epoch 41/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.7007 - acc: 0.7641 - val_loss: 0.5687 - val_acc: 0.8196\n",
      "Epoch 42/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6991 - acc: 0.7651 - val_loss: 0.5380 - val_acc: 0.8236\n",
      "Epoch 43/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6983 - acc: 0.7655 - val_loss: 0.5191 - val_acc: 0.8302\n",
      "Epoch 44/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6891 - acc: 0.7685 - val_loss: 0.5677 - val_acc: 0.8185\n",
      "Epoch 45/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6908 - acc: 0.7669 - val_loss: 0.5300 - val_acc: 0.8283\n",
      "Epoch 46/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6897 - acc: 0.7667 - val_loss: 0.5661 - val_acc: 0.8179\n",
      "Epoch 47/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6862 - acc: 0.7687 - val_loss: 0.5601 - val_acc: 0.8220\n",
      "Epoch 48/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6867 - acc: 0.7702 - val_loss: 0.5534 - val_acc: 0.8221\n",
      "Epoch 49/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6904 - acc: 0.7672 - val_loss: 0.5130 - val_acc: 0.8339\n",
      "Epoch 50/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6854 - acc: 0.7697 - val_loss: 0.5718 - val_acc: 0.8178\n",
      "Epoch 51/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6811 - acc: 0.7713 - val_loss: 0.5508 - val_acc: 0.8252\n",
      "Epoch 52/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6845 - acc: 0.7688 - val_loss: 0.5549 - val_acc: 0.8224\n",
      "Epoch 53/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6721 - acc: 0.7756 - val_loss: 0.5451 - val_acc: 0.8243\n",
      "Epoch 54/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6773 - acc: 0.7714 - val_loss: 0.5272 - val_acc: 0.8327\n",
      "Epoch 55/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6793 - acc: 0.7724 - val_loss: 0.5296 - val_acc: 0.8357\n",
      "Epoch 56/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6764 - acc: 0.7751 - val_loss: 0.4963 - val_acc: 0.8385\n",
      "Epoch 57/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6739 - acc: 0.7732 - val_loss: 0.5256 - val_acc: 0.8310\n",
      "Epoch 58/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6785 - acc: 0.7706 - val_loss: 0.5342 - val_acc: 0.8281\n",
      "Epoch 59/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6726 - acc: 0.7742 - val_loss: 0.5345 - val_acc: 0.8276\n",
      "Epoch 60/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6728 - acc: 0.7761 - val_loss: 0.5119 - val_acc: 0.8342\n",
      "Epoch 61/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6689 - acc: 0.7770 - val_loss: 0.5615 - val_acc: 0.8231\n",
      "Epoch 62/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6711 - acc: 0.7745 - val_loss: 0.5835 - val_acc: 0.8171\n",
      "Epoch 63/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6674 - acc: 0.7749 - val_loss: 0.4907 - val_acc: 0.8382\n",
      "Epoch 64/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6655 - acc: 0.7760 - val_loss: 0.5210 - val_acc: 0.8305\n",
      "Epoch 65/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6610 - acc: 0.7771 - val_loss: 0.4978 - val_acc: 0.8373\n",
      "Epoch 66/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6652 - acc: 0.7794 - val_loss: 0.4988 - val_acc: 0.8372\n",
      "Epoch 67/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6611 - acc: 0.7783 - val_loss: 0.5286 - val_acc: 0.8317\n",
      "Epoch 68/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6653 - acc: 0.7776 - val_loss: 0.5384 - val_acc: 0.8300\n",
      "Epoch 69/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6583 - acc: 0.7777 - val_loss: 0.4920 - val_acc: 0.8400\n",
      "Epoch 70/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6622 - acc: 0.7795 - val_loss: 0.5633 - val_acc: 0.8203\n",
      "Epoch 71/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6607 - acc: 0.7768 - val_loss: 0.5566 - val_acc: 0.8235\n",
      "Epoch 72/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6632 - acc: 0.7769 - val_loss: 0.4984 - val_acc: 0.8398\n",
      "Epoch 73/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6545 - acc: 0.7797 - val_loss: 0.5742 - val_acc: 0.8195\n",
      "Epoch 74/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6584 - acc: 0.7776 - val_loss: 0.5359 - val_acc: 0.8323\n",
      "Epoch 75/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6534 - acc: 0.7803 - val_loss: 0.4882 - val_acc: 0.8413\n",
      "Epoch 76/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6515 - acc: 0.7805 - val_loss: 0.5735 - val_acc: 0.8221\n",
      "Epoch 77/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6600 - acc: 0.7790 - val_loss: 0.5096 - val_acc: 0.8361\n",
      "Epoch 78/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6482 - acc: 0.7829 - val_loss: 0.5092 - val_acc: 0.8336\n",
      "Epoch 79/500\n",
      "lrate:  0.00025\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6565 - acc: 0.7800 - val_loss: 0.4966 - val_acc: 0.8389\n",
      "Epoch 80/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6406 - acc: 0.7853 - val_loss: 0.4675 - val_acc: 0.8473\n",
      "Epoch 81/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6417 - acc: 0.7849 - val_loss: 0.5124 - val_acc: 0.8336\n",
      "Epoch 82/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6370 - acc: 0.7867 - val_loss: 0.5146 - val_acc: 0.8334\n",
      "Epoch 83/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6334 - acc: 0.7873 - val_loss: 0.4823 - val_acc: 0.8451\n",
      "Epoch 84/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6272 - acc: 0.7892 - val_loss: 0.4784 - val_acc: 0.8473\n",
      "Epoch 85/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6296 - acc: 0.7888 - val_loss: 0.5050 - val_acc: 0.8389\n",
      "Epoch 86/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6267 - acc: 0.7900 - val_loss: 0.5167 - val_acc: 0.8353\n",
      "Epoch 87/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6294 - acc: 0.7889 - val_loss: 0.4916 - val_acc: 0.8411\n",
      "Epoch 88/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6276 - acc: 0.7872 - val_loss: 0.5121 - val_acc: 0.8395\n",
      "Epoch 89/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6279 - acc: 0.7882 - val_loss: 0.5035 - val_acc: 0.8392\n",
      "Epoch 90/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6235 - acc: 0.7908 - val_loss: 0.4957 - val_acc: 0.8405\n",
      "Epoch 91/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6236 - acc: 0.7897 - val_loss: 0.5020 - val_acc: 0.8408\n",
      "Epoch 92/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6240 - acc: 0.7909 - val_loss: 0.5001 - val_acc: 0.8397\n",
      "Epoch 93/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6242 - acc: 0.7919 - val_loss: 0.4818 - val_acc: 0.8444\n",
      "Epoch 94/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6242 - acc: 0.7920 - val_loss: 0.4959 - val_acc: 0.8408\n",
      "Epoch 95/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6185 - acc: 0.7927 - val_loss: 0.4692 - val_acc: 0.8485\n",
      "Epoch 96/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6263 - acc: 0.7906 - val_loss: 0.4942 - val_acc: 0.8380\n",
      "Epoch 97/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6199 - acc: 0.7909 - val_loss: 0.5002 - val_acc: 0.8384\n",
      "Epoch 98/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6180 - acc: 0.7930 - val_loss: 0.4674 - val_acc: 0.8479\n",
      "Epoch 99/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6222 - acc: 0.7884 - val_loss: 0.4946 - val_acc: 0.8448\n",
      "Epoch 100/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6177 - acc: 0.7931 - val_loss: 0.4990 - val_acc: 0.8398\n",
      "Epoch 101/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6166 - acc: 0.7938 - val_loss: 0.4984 - val_acc: 0.8400\n",
      "Epoch 102/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6254 - acc: 0.7897 - val_loss: 0.5080 - val_acc: 0.8386\n",
      "Epoch 103/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6212 - acc: 0.7919 - val_loss: 0.4947 - val_acc: 0.8400\n",
      "Epoch 104/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6210 - acc: 0.7929 - val_loss: 0.4641 - val_acc: 0.8495\n",
      "Epoch 105/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6210 - acc: 0.7905 - val_loss: 0.4808 - val_acc: 0.8448\n",
      "Epoch 106/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6191 - acc: 0.7935 - val_loss: 0.4860 - val_acc: 0.8452\n",
      "Epoch 107/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6189 - acc: 0.7925 - val_loss: 0.4873 - val_acc: 0.8457\n",
      "Epoch 108/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6115 - acc: 0.7938 - val_loss: 0.4731 - val_acc: 0.8466\n",
      "Epoch 109/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6095 - acc: 0.7965 - val_loss: 0.4686 - val_acc: 0.8470\n",
      "Epoch 110/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6123 - acc: 0.7933 - val_loss: 0.4881 - val_acc: 0.8421\n",
      "Epoch 111/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6107 - acc: 0.7956 - val_loss: 0.4833 - val_acc: 0.8467\n",
      "Epoch 112/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6104 - acc: 0.7965 - val_loss: 0.4850 - val_acc: 0.8439\n",
      "Epoch 113/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6144 - acc: 0.7948 - val_loss: 0.4700 - val_acc: 0.8460\n",
      "Epoch 114/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6156 - acc: 0.7922 - val_loss: 0.4756 - val_acc: 0.8472\n",
      "Epoch 115/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6145 - acc: 0.7958 - val_loss: 0.4697 - val_acc: 0.8486\n",
      "Epoch 116/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6122 - acc: 0.7969 - val_loss: 0.4720 - val_acc: 0.8486\n",
      "Epoch 117/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6110 - acc: 0.7953 - val_loss: 0.5228 - val_acc: 0.8347\n",
      "Epoch 118/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6096 - acc: 0.7950 - val_loss: 0.4579 - val_acc: 0.8517\n",
      "Epoch 119/500\n",
      "lrate:  0.000125\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6098 - acc: 0.7966 - val_loss: 0.4911 - val_acc: 0.8428\n",
      "Epoch 120/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6094 - acc: 0.7942 - val_loss: 0.4925 - val_acc: 0.8429\n",
      "Epoch 121/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5983 - acc: 0.8003 - val_loss: 0.4816 - val_acc: 0.8460\n",
      "Epoch 122/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6126 - acc: 0.7963 - val_loss: 0.4821 - val_acc: 0.8451\n",
      "Epoch 123/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5995 - acc: 0.7988 - val_loss: 0.4842 - val_acc: 0.8441\n",
      "Epoch 124/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6018 - acc: 0.7989 - val_loss: 0.4951 - val_acc: 0.8439\n",
      "Epoch 125/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5933 - acc: 0.8013 - val_loss: 0.4657 - val_acc: 0.8514\n",
      "Epoch 126/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5980 - acc: 0.7989 - val_loss: 0.4668 - val_acc: 0.8516\n",
      "Epoch 127/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6027 - acc: 0.7989 - val_loss: 0.4750 - val_acc: 0.8476\n",
      "Epoch 128/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5975 - acc: 0.7994 - val_loss: 0.4496 - val_acc: 0.8545\n",
      "Epoch 129/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6051 - acc: 0.7980 - val_loss: 0.4849 - val_acc: 0.8451\n",
      "Epoch 130/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5957 - acc: 0.7993 - val_loss: 0.4638 - val_acc: 0.8500\n",
      "Epoch 131/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6006 - acc: 0.7971 - val_loss: 0.4683 - val_acc: 0.8513\n",
      "Epoch 132/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5973 - acc: 0.8002 - val_loss: 0.4522 - val_acc: 0.8548\n",
      "Epoch 133/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5997 - acc: 0.7989 - val_loss: 0.4649 - val_acc: 0.8517\n",
      "Epoch 134/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6070 - acc: 0.7984 - val_loss: 0.4739 - val_acc: 0.8495\n",
      "Epoch 135/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5971 - acc: 0.7976 - val_loss: 0.4528 - val_acc: 0.8549\n",
      "Epoch 136/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5973 - acc: 0.8007 - val_loss: 0.4618 - val_acc: 0.8522\n",
      "Epoch 137/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6019 - acc: 0.7969 - val_loss: 0.4678 - val_acc: 0.8495\n",
      "Epoch 138/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5945 - acc: 0.8016 - val_loss: 0.4597 - val_acc: 0.8529\n",
      "Epoch 139/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5959 - acc: 0.7997 - val_loss: 0.4829 - val_acc: 0.8464\n",
      "Epoch 140/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5982 - acc: 0.7984 - val_loss: 0.4692 - val_acc: 0.8512\n",
      "Epoch 141/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5892 - acc: 0.8009 - val_loss: 0.4700 - val_acc: 0.8495\n",
      "Epoch 142/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5940 - acc: 0.8015 - val_loss: 0.4795 - val_acc: 0.8478\n",
      "Epoch 143/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5924 - acc: 0.8012 - val_loss: 0.4690 - val_acc: 0.8502\n",
      "Epoch 144/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5955 - acc: 0.8002 - val_loss: 0.4788 - val_acc: 0.8454\n",
      "Epoch 145/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5958 - acc: 0.8005 - val_loss: 0.4709 - val_acc: 0.8487\n",
      "Epoch 146/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6001 - acc: 0.7991 - val_loss: 0.4681 - val_acc: 0.8492\n",
      "Epoch 147/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5903 - acc: 0.8009 - val_loss: 0.4638 - val_acc: 0.8499\n",
      "Epoch 148/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5919 - acc: 0.8011 - val_loss: 0.4907 - val_acc: 0.8455\n",
      "Epoch 149/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5929 - acc: 0.8000 - val_loss: 0.4951 - val_acc: 0.8452\n",
      "Epoch 150/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6021 - acc: 0.7978 - val_loss: 0.4575 - val_acc: 0.8535\n",
      "Epoch 151/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5952 - acc: 0.8006 - val_loss: 0.4599 - val_acc: 0.8526\n",
      "Epoch 152/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5968 - acc: 0.8002 - val_loss: 0.4661 - val_acc: 0.8513\n",
      "Epoch 153/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5939 - acc: 0.8006 - val_loss: 0.4651 - val_acc: 0.8535\n",
      "Epoch 154/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5887 - acc: 0.8032 - val_loss: 0.4629 - val_acc: 0.8518\n",
      "Epoch 155/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5884 - acc: 0.8018 - val_loss: 0.4646 - val_acc: 0.8516\n",
      "Epoch 156/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5968 - acc: 0.7983 - val_loss: 0.4638 - val_acc: 0.8514\n",
      "Epoch 157/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5880 - acc: 0.8035 - val_loss: 0.4557 - val_acc: 0.8515\n",
      "Epoch 158/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5969 - acc: 0.8004 - val_loss: 0.4671 - val_acc: 0.8527\n",
      "Epoch 159/500\n",
      "lrate:  6.25e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5902 - acc: 0.7997 - val_loss: 0.4763 - val_acc: 0.8491\n",
      "Epoch 160/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5904 - acc: 0.8002 - val_loss: 0.4805 - val_acc: 0.8489\n",
      "Epoch 161/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5864 - acc: 0.8016 - val_loss: 0.4603 - val_acc: 0.8541\n",
      "Epoch 162/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5895 - acc: 0.8019 - val_loss: 0.4639 - val_acc: 0.8516\n",
      "Epoch 163/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5917 - acc: 0.7991 - val_loss: 0.4678 - val_acc: 0.8525\n",
      "Epoch 164/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5824 - acc: 0.8037 - val_loss: 0.4584 - val_acc: 0.8540\n",
      "Epoch 165/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5938 - acc: 0.8018 - val_loss: 0.4752 - val_acc: 0.8499\n",
      "Epoch 166/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5859 - acc: 0.8046 - val_loss: 0.4591 - val_acc: 0.8548\n",
      "Epoch 167/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5893 - acc: 0.8011 - val_loss: 0.4686 - val_acc: 0.8519\n",
      "Epoch 168/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5853 - acc: 0.8048 - val_loss: 0.4727 - val_acc: 0.8507\n",
      "Epoch 169/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5858 - acc: 0.8045 - val_loss: 0.4626 - val_acc: 0.8532\n",
      "Epoch 170/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5949 - acc: 0.8013 - val_loss: 0.4584 - val_acc: 0.8532\n",
      "Epoch 171/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5850 - acc: 0.8047 - val_loss: 0.4667 - val_acc: 0.8524\n",
      "Epoch 172/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5867 - acc: 0.8037 - val_loss: 0.4661 - val_acc: 0.8518\n",
      "Epoch 173/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5912 - acc: 0.8026 - val_loss: 0.4552 - val_acc: 0.8541\n",
      "Epoch 174/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5805 - acc: 0.8062 - val_loss: 0.4662 - val_acc: 0.8520\n",
      "Epoch 175/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5803 - acc: 0.8041 - val_loss: 0.4612 - val_acc: 0.8539\n",
      "Epoch 176/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5871 - acc: 0.8044 - val_loss: 0.4494 - val_acc: 0.8553\n",
      "Epoch 177/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5846 - acc: 0.8046 - val_loss: 0.4665 - val_acc: 0.8524\n",
      "Epoch 178/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5876 - acc: 0.8040 - val_loss: 0.4602 - val_acc: 0.8538\n",
      "Epoch 179/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5794 - acc: 0.8057 - val_loss: 0.4570 - val_acc: 0.8546\n",
      "Epoch 180/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5892 - acc: 0.8025 - val_loss: 0.4703 - val_acc: 0.8519\n",
      "Epoch 181/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5825 - acc: 0.8050 - val_loss: 0.4646 - val_acc: 0.8536\n",
      "Epoch 182/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5774 - acc: 0.8065 - val_loss: 0.4616 - val_acc: 0.8533\n",
      "Epoch 183/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5846 - acc: 0.8035 - val_loss: 0.4654 - val_acc: 0.8526\n",
      "Epoch 184/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5865 - acc: 0.8039 - val_loss: 0.4552 - val_acc: 0.8554\n",
      "Epoch 185/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5886 - acc: 0.8018 - val_loss: 0.4592 - val_acc: 0.8530\n",
      "Epoch 186/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5858 - acc: 0.8055 - val_loss: 0.4721 - val_acc: 0.8510\n",
      "Epoch 187/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5789 - acc: 0.8065 - val_loss: 0.4618 - val_acc: 0.8532\n",
      "Epoch 188/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5859 - acc: 0.8037 - val_loss: 0.4601 - val_acc: 0.8549\n",
      "Epoch 189/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5842 - acc: 0.8036 - val_loss: 0.4583 - val_acc: 0.8543\n",
      "Epoch 190/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5766 - acc: 0.8069 - val_loss: 0.4570 - val_acc: 0.8555\n",
      "Epoch 191/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5798 - acc: 0.8057 - val_loss: 0.4643 - val_acc: 0.8537\n",
      "Epoch 192/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5813 - acc: 0.8044 - val_loss: 0.4726 - val_acc: 0.8517\n",
      "Epoch 193/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5826 - acc: 0.8042 - val_loss: 0.4620 - val_acc: 0.8538\n",
      "Epoch 194/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5793 - acc: 0.8053 - val_loss: 0.4530 - val_acc: 0.8563\n",
      "Epoch 195/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5868 - acc: 0.8032 - val_loss: 0.4680 - val_acc: 0.8537\n",
      "Epoch 196/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5778 - acc: 0.8055 - val_loss: 0.4668 - val_acc: 0.8539\n",
      "Epoch 197/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5820 - acc: 0.8063 - val_loss: 0.4606 - val_acc: 0.8555\n",
      "Epoch 198/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5804 - acc: 0.8055 - val_loss: 0.4599 - val_acc: 0.8554\n",
      "Epoch 199/500\n",
      "lrate:  3.125e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5850 - acc: 0.8052 - val_loss: 0.4560 - val_acc: 0.8546\n",
      "Epoch 200/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5760 - acc: 0.8064 - val_loss: 0.4633 - val_acc: 0.8540\n",
      "Epoch 201/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5830 - acc: 0.8057 - val_loss: 0.4578 - val_acc: 0.8550\n",
      "Epoch 202/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5828 - acc: 0.8046 - val_loss: 0.4559 - val_acc: 0.8564\n",
      "Epoch 203/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5893 - acc: 0.8013 - val_loss: 0.4606 - val_acc: 0.8542\n",
      "Epoch 204/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5799 - acc: 0.8068 - val_loss: 0.4598 - val_acc: 0.8546\n",
      "Epoch 205/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5810 - acc: 0.8059 - val_loss: 0.4570 - val_acc: 0.8547\n",
      "Epoch 206/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5825 - acc: 0.8047 - val_loss: 0.4563 - val_acc: 0.8546\n",
      "Epoch 207/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5771 - acc: 0.8061 - val_loss: 0.4621 - val_acc: 0.8534\n",
      "Epoch 208/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5794 - acc: 0.8056 - val_loss: 0.4576 - val_acc: 0.8539\n",
      "Epoch 209/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5768 - acc: 0.8058 - val_loss: 0.4590 - val_acc: 0.8549\n",
      "Epoch 210/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5782 - acc: 0.8057 - val_loss: 0.4547 - val_acc: 0.8549\n",
      "Epoch 211/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5782 - acc: 0.8065 - val_loss: 0.4665 - val_acc: 0.8515\n",
      "Epoch 212/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5767 - acc: 0.8051 - val_loss: 0.4520 - val_acc: 0.8558\n",
      "Epoch 213/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5850 - acc: 0.8038 - val_loss: 0.4639 - val_acc: 0.8518\n",
      "Epoch 214/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5866 - acc: 0.8024 - val_loss: 0.4574 - val_acc: 0.8543\n",
      "Epoch 215/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5845 - acc: 0.8046 - val_loss: 0.4601 - val_acc: 0.8534\n",
      "Epoch 216/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5767 - acc: 0.8084 - val_loss: 0.4561 - val_acc: 0.8546\n",
      "Epoch 217/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5803 - acc: 0.8072 - val_loss: 0.4612 - val_acc: 0.8538\n",
      "Epoch 218/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5785 - acc: 0.8074 - val_loss: 0.4560 - val_acc: 0.8543\n",
      "Epoch 219/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5756 - acc: 0.8078 - val_loss: 0.4576 - val_acc: 0.8554\n",
      "Epoch 220/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5786 - acc: 0.8065 - val_loss: 0.4571 - val_acc: 0.8557\n",
      "Epoch 221/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5798 - acc: 0.8045 - val_loss: 0.4561 - val_acc: 0.8544\n",
      "Epoch 222/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5729 - acc: 0.8081 - val_loss: 0.4526 - val_acc: 0.8562\n",
      "Epoch 223/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5744 - acc: 0.8090 - val_loss: 0.4578 - val_acc: 0.8557\n",
      "Epoch 224/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5824 - acc: 0.8044 - val_loss: 0.4621 - val_acc: 0.8538\n",
      "Epoch 225/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5786 - acc: 0.8065 - val_loss: 0.4542 - val_acc: 0.8559\n",
      "Epoch 226/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5755 - acc: 0.8052 - val_loss: 0.4641 - val_acc: 0.8533\n",
      "Epoch 227/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5725 - acc: 0.8081 - val_loss: 0.4586 - val_acc: 0.8553\n",
      "Epoch 228/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5774 - acc: 0.8053 - val_loss: 0.4498 - val_acc: 0.8563\n",
      "Epoch 229/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5797 - acc: 0.8056 - val_loss: 0.4635 - val_acc: 0.8535\n",
      "Epoch 230/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5753 - acc: 0.8065 - val_loss: 0.4578 - val_acc: 0.8542\n",
      "Epoch 231/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5781 - acc: 0.8079 - val_loss: 0.4509 - val_acc: 0.8575\n",
      "Epoch 232/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5752 - acc: 0.8063 - val_loss: 0.4590 - val_acc: 0.8551\n",
      "Epoch 233/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5742 - acc: 0.8056 - val_loss: 0.4594 - val_acc: 0.8552\n",
      "Epoch 234/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5753 - acc: 0.8079 - val_loss: 0.4598 - val_acc: 0.8554\n",
      "Epoch 235/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5779 - acc: 0.8071 - val_loss: 0.4570 - val_acc: 0.8550\n",
      "Epoch 236/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5701 - acc: 0.8066 - val_loss: 0.4573 - val_acc: 0.8548\n",
      "Epoch 237/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5779 - acc: 0.8075 - val_loss: 0.4630 - val_acc: 0.8538\n",
      "Epoch 238/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5765 - acc: 0.8065 - val_loss: 0.4544 - val_acc: 0.8560\n",
      "Epoch 239/500\n",
      "lrate:  1.5625e-05\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5818 - acc: 0.8043 - val_loss: 0.4561 - val_acc: 0.8557\n",
      "Epoch 240/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5792 - acc: 0.8050 - val_loss: 0.4559 - val_acc: 0.8551\n",
      "Epoch 241/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5768 - acc: 0.8074 - val_loss: 0.4542 - val_acc: 0.8549\n",
      "Epoch 242/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5748 - acc: 0.8068 - val_loss: 0.4562 - val_acc: 0.8554\n",
      "Epoch 243/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5757 - acc: 0.8068 - val_loss: 0.4556 - val_acc: 0.8556\n",
      "Epoch 244/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5767 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8554\n",
      "Epoch 245/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5864 - acc: 0.8052 - val_loss: 0.4567 - val_acc: 0.8553\n",
      "Epoch 246/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5781 - acc: 0.8075 - val_loss: 0.4621 - val_acc: 0.8535\n",
      "Epoch 247/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5819 - acc: 0.8039 - val_loss: 0.4603 - val_acc: 0.8551\n",
      "Epoch 248/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5773 - acc: 0.8050 - val_loss: 0.4579 - val_acc: 0.8554\n",
      "Epoch 249/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5781 - acc: 0.8082 - val_loss: 0.4609 - val_acc: 0.8547\n",
      "Epoch 250/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5736 - acc: 0.8100 - val_loss: 0.4601 - val_acc: 0.8535\n",
      "Epoch 251/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5742 - acc: 0.8069 - val_loss: 0.4515 - val_acc: 0.8577\n",
      "Epoch 252/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5830 - acc: 0.8062 - val_loss: 0.4536 - val_acc: 0.8551\n",
      "Epoch 253/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5762 - acc: 0.8081 - val_loss: 0.4553 - val_acc: 0.8552\n",
      "Epoch 254/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5662 - acc: 0.8110 - val_loss: 0.4624 - val_acc: 0.8545\n",
      "Epoch 255/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5750 - acc: 0.8086 - val_loss: 0.4610 - val_acc: 0.8542\n",
      "Epoch 256/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5712 - acc: 0.8094 - val_loss: 0.4562 - val_acc: 0.8549\n",
      "Epoch 257/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5752 - acc: 0.8060 - val_loss: 0.4684 - val_acc: 0.8527\n",
      "Epoch 258/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5697 - acc: 0.8107 - val_loss: 0.4625 - val_acc: 0.8534\n",
      "Epoch 259/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5675 - acc: 0.8106 - val_loss: 0.4516 - val_acc: 0.8574\n",
      "Epoch 260/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5736 - acc: 0.8066 - val_loss: 0.4599 - val_acc: 0.8551\n",
      "Epoch 261/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5722 - acc: 0.8086 - val_loss: 0.4584 - val_acc: 0.8552\n",
      "Epoch 262/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5711 - acc: 0.8105 - val_loss: 0.4553 - val_acc: 0.8560\n",
      "Epoch 263/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5700 - acc: 0.8100 - val_loss: 0.4549 - val_acc: 0.8557\n",
      "Epoch 264/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5724 - acc: 0.8070 - val_loss: 0.4568 - val_acc: 0.8541\n",
      "Epoch 265/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5840 - acc: 0.8057 - val_loss: 0.4576 - val_acc: 0.8554\n",
      "Epoch 266/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5726 - acc: 0.8091 - val_loss: 0.4531 - val_acc: 0.8564\n",
      "Epoch 267/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5694 - acc: 0.8101 - val_loss: 0.4541 - val_acc: 0.8557\n",
      "Epoch 268/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5769 - acc: 0.8064 - val_loss: 0.4502 - val_acc: 0.8569\n",
      "Epoch 269/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5723 - acc: 0.8074 - val_loss: 0.4514 - val_acc: 0.8564\n",
      "Epoch 270/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5785 - acc: 0.8074 - val_loss: 0.4582 - val_acc: 0.8544\n",
      "Epoch 271/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5758 - acc: 0.8077 - val_loss: 0.4536 - val_acc: 0.8560\n",
      "Epoch 272/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5739 - acc: 0.8090 - val_loss: 0.4561 - val_acc: 0.8555\n",
      "Epoch 273/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5690 - acc: 0.8091 - val_loss: 0.4604 - val_acc: 0.8546\n",
      "Epoch 274/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5724 - acc: 0.8059 - val_loss: 0.4573 - val_acc: 0.8555\n",
      "Epoch 275/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5789 - acc: 0.8081 - val_loss: 0.4497 - val_acc: 0.8577\n",
      "Epoch 276/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5643 - acc: 0.8111 - val_loss: 0.4537 - val_acc: 0.8562\n",
      "Epoch 277/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5789 - acc: 0.8077 - val_loss: 0.4537 - val_acc: 0.8552\n",
      "Epoch 278/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5792 - acc: 0.8060 - val_loss: 0.4565 - val_acc: 0.8557\n",
      "Epoch 279/500\n",
      "lrate:  7.8125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5744 - acc: 0.8064 - val_loss: 0.4561 - val_acc: 0.8555\n",
      "Epoch 280/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5744 - acc: 0.8065 - val_loss: 0.4540 - val_acc: 0.8567\n",
      "Epoch 281/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5677 - acc: 0.8109 - val_loss: 0.4603 - val_acc: 0.8547\n",
      "Epoch 282/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5765 - acc: 0.8086 - val_loss: 0.4573 - val_acc: 0.8551\n",
      "Epoch 283/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5773 - acc: 0.8080 - val_loss: 0.4597 - val_acc: 0.8553\n",
      "Epoch 284/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5710 - acc: 0.8101 - val_loss: 0.4562 - val_acc: 0.8561\n",
      "Epoch 285/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5773 - acc: 0.8064 - val_loss: 0.4504 - val_acc: 0.8569\n",
      "Epoch 286/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5688 - acc: 0.8077 - val_loss: 0.4570 - val_acc: 0.8558\n",
      "Epoch 287/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5805 - acc: 0.8067 - val_loss: 0.4563 - val_acc: 0.8554\n",
      "Epoch 288/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5774 - acc: 0.8078 - val_loss: 0.4567 - val_acc: 0.8539\n",
      "Epoch 289/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5736 - acc: 0.8076 - val_loss: 0.4524 - val_acc: 0.8571\n",
      "Epoch 290/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5728 - acc: 0.8064 - val_loss: 0.4533 - val_acc: 0.8553\n",
      "Epoch 291/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5756 - acc: 0.8070 - val_loss: 0.4535 - val_acc: 0.8569\n",
      "Epoch 292/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5767 - acc: 0.8062 - val_loss: 0.4588 - val_acc: 0.8533\n",
      "Epoch 293/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5730 - acc: 0.8074 - val_loss: 0.4549 - val_acc: 0.8556\n",
      "Epoch 294/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5760 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8563\n",
      "Epoch 295/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5750 - acc: 0.8075 - val_loss: 0.4561 - val_acc: 0.8562\n",
      "Epoch 296/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5777 - acc: 0.8065 - val_loss: 0.4615 - val_acc: 0.8543\n",
      "Epoch 297/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5795 - acc: 0.8041 - val_loss: 0.4556 - val_acc: 0.8560\n",
      "Epoch 298/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5766 - acc: 0.8058 - val_loss: 0.4639 - val_acc: 0.8530\n",
      "Epoch 299/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5732 - acc: 0.8072 - val_loss: 0.4553 - val_acc: 0.8565\n",
      "Epoch 300/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5701 - acc: 0.8106 - val_loss: 0.4532 - val_acc: 0.8567\n",
      "Epoch 301/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5703 - acc: 0.8066 - val_loss: 0.4584 - val_acc: 0.8559\n",
      "Epoch 302/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5779 - acc: 0.8081 - val_loss: 0.4611 - val_acc: 0.8547\n",
      "Epoch 303/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5672 - acc: 0.8103 - val_loss: 0.4572 - val_acc: 0.8552\n",
      "Epoch 304/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5744 - acc: 0.8072 - val_loss: 0.4559 - val_acc: 0.8555\n",
      "Epoch 305/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5761 - acc: 0.8075 - val_loss: 0.4553 - val_acc: 0.8550\n",
      "Epoch 306/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5724 - acc: 0.8103 - val_loss: 0.4565 - val_acc: 0.8553\n",
      "Epoch 307/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5698 - acc: 0.8073 - val_loss: 0.4518 - val_acc: 0.8559\n",
      "Epoch 308/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5751 - acc: 0.8060 - val_loss: 0.4565 - val_acc: 0.8555\n",
      "Epoch 309/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5739 - acc: 0.8094 - val_loss: 0.4590 - val_acc: 0.8556\n",
      "Epoch 310/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5779 - acc: 0.8065 - val_loss: 0.4506 - val_acc: 0.8568\n",
      "Epoch 311/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5779 - acc: 0.8067 - val_loss: 0.4556 - val_acc: 0.8545\n",
      "Epoch 312/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5714 - acc: 0.8078 - val_loss: 0.4608 - val_acc: 0.8540\n",
      "Epoch 313/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5686 - acc: 0.8109 - val_loss: 0.4601 - val_acc: 0.8532\n",
      "Epoch 314/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5689 - acc: 0.8064 - val_loss: 0.4542 - val_acc: 0.8557\n",
      "Epoch 315/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5709 - acc: 0.8076 - val_loss: 0.4614 - val_acc: 0.8538\n",
      "Epoch 316/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5736 - acc: 0.8074 - val_loss: 0.4593 - val_acc: 0.8551\n",
      "Epoch 317/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5760 - acc: 0.8066 - val_loss: 0.4617 - val_acc: 0.8543\n",
      "Epoch 318/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5677 - acc: 0.8103 - val_loss: 0.4604 - val_acc: 0.8552\n",
      "Epoch 319/500\n",
      "lrate:  3.90625e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5738 - acc: 0.8074 - val_loss: 0.4582 - val_acc: 0.8559\n",
      "Epoch 320/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5755 - acc: 0.8102 - val_loss: 0.4605 - val_acc: 0.8544\n",
      "Epoch 321/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5755 - acc: 0.8072 - val_loss: 0.4626 - val_acc: 0.8534\n",
      "Epoch 322/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5802 - acc: 0.8069 - val_loss: 0.4534 - val_acc: 0.8563\n",
      "Epoch 323/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5779 - acc: 0.8088 - val_loss: 0.4546 - val_acc: 0.8563\n",
      "Epoch 324/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5755 - acc: 0.8068 - val_loss: 0.4582 - val_acc: 0.8546\n",
      "Epoch 325/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5773 - acc: 0.8033 - val_loss: 0.4533 - val_acc: 0.8563\n",
      "Epoch 326/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5796 - acc: 0.8061 - val_loss: 0.4556 - val_acc: 0.8562\n",
      "Epoch 327/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5721 - acc: 0.8086 - val_loss: 0.4558 - val_acc: 0.8557\n",
      "Epoch 328/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5710 - acc: 0.8092 - val_loss: 0.4534 - val_acc: 0.8567\n",
      "Epoch 329/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5776 - acc: 0.8062 - val_loss: 0.4530 - val_acc: 0.8559\n",
      "Epoch 330/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5693 - acc: 0.8082 - val_loss: 0.4592 - val_acc: 0.8551\n",
      "Epoch 331/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5673 - acc: 0.8085 - val_loss: 0.4588 - val_acc: 0.8550\n",
      "Epoch 332/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5762 - acc: 0.8096 - val_loss: 0.4612 - val_acc: 0.8545\n",
      "Epoch 333/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5741 - acc: 0.8065 - val_loss: 0.4560 - val_acc: 0.8573\n",
      "Epoch 334/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5714 - acc: 0.8083 - val_loss: 0.4583 - val_acc: 0.8558\n",
      "Epoch 335/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5739 - acc: 0.8080 - val_loss: 0.4521 - val_acc: 0.8574\n",
      "Epoch 336/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5720 - acc: 0.8075 - val_loss: 0.4611 - val_acc: 0.8544\n",
      "Epoch 337/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5735 - acc: 0.8087 - val_loss: 0.4520 - val_acc: 0.8568\n",
      "Epoch 338/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5668 - acc: 0.8101 - val_loss: 0.4531 - val_acc: 0.8563\n",
      "Epoch 339/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5746 - acc: 0.8105 - val_loss: 0.4592 - val_acc: 0.8556\n",
      "Epoch 340/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5685 - acc: 0.8082 - val_loss: 0.4567 - val_acc: 0.8554\n",
      "Epoch 341/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5715 - acc: 0.8089 - val_loss: 0.4606 - val_acc: 0.8551\n",
      "Epoch 342/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5762 - acc: 0.8084 - val_loss: 0.4573 - val_acc: 0.8559\n",
      "Epoch 343/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5713 - acc: 0.8072 - val_loss: 0.4589 - val_acc: 0.8546\n",
      "Epoch 344/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5744 - acc: 0.8095 - val_loss: 0.4610 - val_acc: 0.8545\n",
      "Epoch 345/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5730 - acc: 0.8078 - val_loss: 0.4585 - val_acc: 0.8550\n",
      "Epoch 346/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5716 - acc: 0.8090 - val_loss: 0.4542 - val_acc: 0.8565\n",
      "Epoch 347/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5779 - acc: 0.8058 - val_loss: 0.4528 - val_acc: 0.8576\n",
      "Epoch 348/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5695 - acc: 0.8096 - val_loss: 0.4582 - val_acc: 0.8549\n",
      "Epoch 349/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5751 - acc: 0.8092 - val_loss: 0.4574 - val_acc: 0.8554\n",
      "Epoch 350/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5835 - acc: 0.8040 - val_loss: 0.4601 - val_acc: 0.8541\n",
      "Epoch 351/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5704 - acc: 0.8089 - val_loss: 0.4669 - val_acc: 0.8528\n",
      "Epoch 352/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5757 - acc: 0.8085 - val_loss: 0.4605 - val_acc: 0.8554\n",
      "Epoch 353/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5745 - acc: 0.8075 - val_loss: 0.4610 - val_acc: 0.8543\n",
      "Epoch 354/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5732 - acc: 0.8080 - val_loss: 0.4565 - val_acc: 0.8556\n",
      "Epoch 355/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5725 - acc: 0.8085 - val_loss: 0.4571 - val_acc: 0.8562\n",
      "Epoch 356/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5681 - acc: 0.8089 - val_loss: 0.4506 - val_acc: 0.8576\n",
      "Epoch 357/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5812 - acc: 0.8036 - val_loss: 0.4569 - val_acc: 0.8561\n",
      "Epoch 358/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5719 - acc: 0.8085 - val_loss: 0.4579 - val_acc: 0.8561\n",
      "Epoch 359/500\n",
      "lrate:  1.953125e-06\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5826 - acc: 0.8050 - val_loss: 0.4516 - val_acc: 0.8573\n",
      "Epoch 360/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5719 - acc: 0.8101 - val_loss: 0.4555 - val_acc: 0.8565\n",
      "Epoch 361/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5683 - acc: 0.8093 - val_loss: 0.4591 - val_acc: 0.8557\n",
      "Epoch 362/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5729 - acc: 0.8101 - val_loss: 0.4601 - val_acc: 0.8545\n",
      "Epoch 363/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5800 - acc: 0.8067 - val_loss: 0.4568 - val_acc: 0.8555\n",
      "Epoch 364/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5693 - acc: 0.8103 - val_loss: 0.4552 - val_acc: 0.8567\n",
      "Epoch 365/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5708 - acc: 0.8085 - val_loss: 0.4594 - val_acc: 0.8545\n",
      "Epoch 366/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5709 - acc: 0.8097 - val_loss: 0.4610 - val_acc: 0.8546\n",
      "Epoch 367/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5727 - acc: 0.8071 - val_loss: 0.4580 - val_acc: 0.8559\n",
      "Epoch 368/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5739 - acc: 0.8091 - val_loss: 0.4602 - val_acc: 0.8543\n",
      "Epoch 369/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5690 - acc: 0.8082 - val_loss: 0.4624 - val_acc: 0.8546\n",
      "Epoch 370/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5743 - acc: 0.8079 - val_loss: 0.4548 - val_acc: 0.8553\n",
      "Epoch 371/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5740 - acc: 0.8067 - val_loss: 0.4548 - val_acc: 0.8560\n",
      "Epoch 372/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5713 - acc: 0.8085 - val_loss: 0.4570 - val_acc: 0.8554\n",
      "Epoch 373/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5760 - acc: 0.8061 - val_loss: 0.4540 - val_acc: 0.8562\n",
      "Epoch 374/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5682 - acc: 0.8098 - val_loss: 0.4593 - val_acc: 0.8551\n",
      "Epoch 375/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5746 - acc: 0.8088 - val_loss: 0.4559 - val_acc: 0.8556\n",
      "Epoch 376/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5675 - acc: 0.8093 - val_loss: 0.4545 - val_acc: 0.8563\n",
      "Epoch 377/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5720 - acc: 0.8085 - val_loss: 0.4587 - val_acc: 0.8550\n",
      "Epoch 378/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5737 - acc: 0.8094 - val_loss: 0.4549 - val_acc: 0.8560\n",
      "Epoch 379/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5713 - acc: 0.8095 - val_loss: 0.4613 - val_acc: 0.8535\n",
      "Epoch 380/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.5748 - acc: 0.8079 - val_loss: 0.4579 - val_acc: 0.8556\n",
      "Epoch 381/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5782 - acc: 0.8071 - val_loss: 0.4594 - val_acc: 0.8549\n",
      "Epoch 382/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5735 - acc: 0.8088 - val_loss: 0.4567 - val_acc: 0.8560\n",
      "Epoch 383/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5663 - acc: 0.8104 - val_loss: 0.4552 - val_acc: 0.8556\n",
      "Epoch 384/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5715 - acc: 0.8076 - val_loss: 0.4568 - val_acc: 0.8546\n",
      "Epoch 385/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5735 - acc: 0.8073 - val_loss: 0.4550 - val_acc: 0.8566\n",
      "Epoch 386/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5723 - acc: 0.8096 - val_loss: 0.4600 - val_acc: 0.8547\n",
      "Epoch 387/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5743 - acc: 0.8078 - val_loss: 0.4584 - val_acc: 0.8553\n",
      "Epoch 388/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5708 - acc: 0.8093 - val_loss: 0.4532 - val_acc: 0.8564\n",
      "Epoch 389/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5735 - acc: 0.8072 - val_loss: 0.4568 - val_acc: 0.8556\n",
      "Epoch 390/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5710 - acc: 0.8097 - val_loss: 0.4590 - val_acc: 0.8552\n",
      "Epoch 391/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5746 - acc: 0.8069 - val_loss: 0.4635 - val_acc: 0.8546\n",
      "Epoch 392/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5692 - acc: 0.8100 - val_loss: 0.4561 - val_acc: 0.8554\n",
      "Epoch 393/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5699 - acc: 0.8088 - val_loss: 0.4610 - val_acc: 0.8544\n",
      "Epoch 394/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5734 - acc: 0.8070 - val_loss: 0.4629 - val_acc: 0.8534\n",
      "Epoch 395/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5746 - acc: 0.8076 - val_loss: 0.4588 - val_acc: 0.8538\n",
      "Epoch 396/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5688 - acc: 0.8085 - val_loss: 0.4565 - val_acc: 0.8555\n",
      "Epoch 397/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5725 - acc: 0.8079 - val_loss: 0.4569 - val_acc: 0.8550\n",
      "Epoch 398/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5780 - acc: 0.8067 - val_loss: 0.4586 - val_acc: 0.8547\n",
      "Epoch 399/500\n",
      "lrate:  9.765625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5757 - acc: 0.8072 - val_loss: 0.4560 - val_acc: 0.8558\n",
      "Epoch 400/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5685 - acc: 0.8091 - val_loss: 0.4582 - val_acc: 0.8547\n",
      "Epoch 401/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5710 - acc: 0.8081 - val_loss: 0.4580 - val_acc: 0.8547\n",
      "Epoch 402/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5715 - acc: 0.8077 - val_loss: 0.4555 - val_acc: 0.8556\n",
      "Epoch 403/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5734 - acc: 0.8086 - val_loss: 0.4600 - val_acc: 0.8552\n",
      "Epoch 404/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5717 - acc: 0.8083 - val_loss: 0.4584 - val_acc: 0.8554\n",
      "Epoch 405/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5658 - acc: 0.8098 - val_loss: 0.4574 - val_acc: 0.8558\n",
      "Epoch 406/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5783 - acc: 0.8090 - val_loss: 0.4538 - val_acc: 0.8560\n",
      "Epoch 407/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5725 - acc: 0.8088 - val_loss: 0.4536 - val_acc: 0.8561\n",
      "Epoch 408/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5751 - acc: 0.8079 - val_loss: 0.4589 - val_acc: 0.8548\n",
      "Epoch 409/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5739 - acc: 0.8066 - val_loss: 0.4525 - val_acc: 0.8567\n",
      "Epoch 410/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5736 - acc: 0.8068 - val_loss: 0.4553 - val_acc: 0.8565\n",
      "Epoch 411/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5771 - acc: 0.8088 - val_loss: 0.4566 - val_acc: 0.8558\n",
      "Epoch 412/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5735 - acc: 0.8068 - val_loss: 0.4629 - val_acc: 0.8543\n",
      "Epoch 413/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5790 - acc: 0.8055 - val_loss: 0.4624 - val_acc: 0.8544\n",
      "Epoch 414/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5725 - acc: 0.8086 - val_loss: 0.4552 - val_acc: 0.8560\n",
      "Epoch 415/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5660 - acc: 0.8098 - val_loss: 0.4548 - val_acc: 0.8562\n",
      "Epoch 416/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5728 - acc: 0.8120 - val_loss: 0.4557 - val_acc: 0.8567\n",
      "Epoch 417/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5745 - acc: 0.8063 - val_loss: 0.4589 - val_acc: 0.8549\n",
      "Epoch 418/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5743 - acc: 0.8081 - val_loss: 0.4578 - val_acc: 0.8559\n",
      "Epoch 419/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5703 - acc: 0.8110 - val_loss: 0.4526 - val_acc: 0.8565\n",
      "Epoch 420/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5724 - acc: 0.8080 - val_loss: 0.4578 - val_acc: 0.8558\n",
      "Epoch 421/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5698 - acc: 0.8099 - val_loss: 0.4564 - val_acc: 0.8555\n",
      "Epoch 422/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5788 - acc: 0.8069 - val_loss: 0.4563 - val_acc: 0.8560\n",
      "Epoch 423/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5729 - acc: 0.8088 - val_loss: 0.4612 - val_acc: 0.8554\n",
      "Epoch 424/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5671 - acc: 0.8084 - val_loss: 0.4550 - val_acc: 0.8570\n",
      "Epoch 425/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5711 - acc: 0.8100 - val_loss: 0.4560 - val_acc: 0.8562\n",
      "Epoch 426/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5792 - acc: 0.8050 - val_loss: 0.4545 - val_acc: 0.8578\n",
      "Epoch 427/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5745 - acc: 0.8084 - val_loss: 0.4581 - val_acc: 0.8549\n",
      "Epoch 428/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5787 - acc: 0.8046 - val_loss: 0.4551 - val_acc: 0.8559\n",
      "Epoch 429/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5734 - acc: 0.8072 - val_loss: 0.4521 - val_acc: 0.8569\n",
      "Epoch 430/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5738 - acc: 0.8069 - val_loss: 0.4595 - val_acc: 0.8552\n",
      "Epoch 431/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5744 - acc: 0.8069 - val_loss: 0.4535 - val_acc: 0.8563\n",
      "Epoch 432/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5688 - acc: 0.8096 - val_loss: 0.4543 - val_acc: 0.8565\n",
      "Epoch 433/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5754 - acc: 0.8065 - val_loss: 0.4577 - val_acc: 0.8559\n",
      "Epoch 434/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5728 - acc: 0.8069 - val_loss: 0.4539 - val_acc: 0.8565\n",
      "Epoch 435/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5755 - acc: 0.8086 - val_loss: 0.4596 - val_acc: 0.8545\n",
      "Epoch 436/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5717 - acc: 0.8085 - val_loss: 0.4578 - val_acc: 0.8547\n",
      "Epoch 437/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5759 - acc: 0.8065 - val_loss: 0.4588 - val_acc: 0.8545\n",
      "Epoch 438/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5739 - acc: 0.8059 - val_loss: 0.4578 - val_acc: 0.8557\n",
      "Epoch 439/500\n",
      "lrate:  4.8828125e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5687 - acc: 0.8091 - val_loss: 0.4538 - val_acc: 0.8568\n",
      "Epoch 440/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5683 - acc: 0.8088 - val_loss: 0.4602 - val_acc: 0.8549\n",
      "Epoch 441/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5712 - acc: 0.8069 - val_loss: 0.4549 - val_acc: 0.8563\n",
      "Epoch 442/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5778 - acc: 0.8079 - val_loss: 0.4569 - val_acc: 0.8561\n",
      "Epoch 443/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5752 - acc: 0.8085 - val_loss: 0.4568 - val_acc: 0.8558\n",
      "Epoch 444/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5801 - acc: 0.8073 - val_loss: 0.4542 - val_acc: 0.8558\n",
      "Epoch 445/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5645 - acc: 0.8085 - val_loss: 0.4566 - val_acc: 0.8557\n",
      "Epoch 446/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5737 - acc: 0.8076 - val_loss: 0.4538 - val_acc: 0.8565\n",
      "Epoch 447/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5648 - acc: 0.8108 - val_loss: 0.4592 - val_acc: 0.8545\n",
      "Epoch 448/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5745 - acc: 0.8083 - val_loss: 0.4568 - val_acc: 0.8560\n",
      "Epoch 449/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5732 - acc: 0.8082 - val_loss: 0.4549 - val_acc: 0.8553\n",
      "Epoch 450/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5713 - acc: 0.8094 - val_loss: 0.4605 - val_acc: 0.8538\n",
      "Epoch 451/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5692 - acc: 0.8094 - val_loss: 0.4523 - val_acc: 0.8566\n",
      "Epoch 452/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5742 - acc: 0.8078 - val_loss: 0.4515 - val_acc: 0.8566\n",
      "Epoch 453/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5740 - acc: 0.8075 - val_loss: 0.4558 - val_acc: 0.8558\n",
      "Epoch 454/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5690 - acc: 0.8107 - val_loss: 0.4587 - val_acc: 0.8544\n",
      "Epoch 455/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5692 - acc: 0.8119 - val_loss: 0.4509 - val_acc: 0.8564\n",
      "Epoch 456/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5725 - acc: 0.8072 - val_loss: 0.4601 - val_acc: 0.8546\n",
      "Epoch 457/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5698 - acc: 0.8098 - val_loss: 0.4613 - val_acc: 0.8548\n",
      "Epoch 458/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5688 - acc: 0.8097 - val_loss: 0.4630 - val_acc: 0.8537\n",
      "Epoch 459/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5771 - acc: 0.8058 - val_loss: 0.4642 - val_acc: 0.8533\n",
      "Epoch 460/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5779 - acc: 0.8049 - val_loss: 0.4551 - val_acc: 0.8563\n",
      "Epoch 461/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5730 - acc: 0.8077 - val_loss: 0.4638 - val_acc: 0.8539\n",
      "Epoch 462/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5715 - acc: 0.8096 - val_loss: 0.4527 - val_acc: 0.8568\n",
      "Epoch 463/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5714 - acc: 0.8076 - val_loss: 0.4606 - val_acc: 0.8540\n",
      "Epoch 464/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5746 - acc: 0.8068 - val_loss: 0.4576 - val_acc: 0.8550\n",
      "Epoch 465/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5718 - acc: 0.8075 - val_loss: 0.4573 - val_acc: 0.8555\n",
      "Epoch 466/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5751 - acc: 0.8075 - val_loss: 0.4572 - val_acc: 0.8555\n",
      "Epoch 467/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5741 - acc: 0.8081 - val_loss: 0.4603 - val_acc: 0.8553\n",
      "Epoch 468/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5723 - acc: 0.8092 - val_loss: 0.4570 - val_acc: 0.8566\n",
      "Epoch 469/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 16ms/step - loss: 0.5742 - acc: 0.8088 - val_loss: 0.4580 - val_acc: 0.8545\n",
      "Epoch 470/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5743 - acc: 0.8082 - val_loss: 0.4605 - val_acc: 0.8544\n",
      "Epoch 471/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5665 - acc: 0.8081 - val_loss: 0.4563 - val_acc: 0.8571\n",
      "Epoch 472/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5752 - acc: 0.8076 - val_loss: 0.4551 - val_acc: 0.8562\n",
      "Epoch 473/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5750 - acc: 0.8076 - val_loss: 0.4543 - val_acc: 0.8561\n",
      "Epoch 474/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5753 - acc: 0.8088 - val_loss: 0.4573 - val_acc: 0.8552\n",
      "Epoch 475/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5791 - acc: 0.8042 - val_loss: 0.4606 - val_acc: 0.8546\n",
      "Epoch 476/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5763 - acc: 0.8075 - val_loss: 0.4611 - val_acc: 0.8544\n",
      "Epoch 477/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5650 - acc: 0.8103 - val_loss: 0.4578 - val_acc: 0.8542\n",
      "Epoch 478/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5774 - acc: 0.8083 - val_loss: 0.4597 - val_acc: 0.8544\n",
      "Epoch 479/500\n",
      "lrate:  2.44140625e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5728 - acc: 0.8084 - val_loss: 0.4540 - val_acc: 0.8563\n",
      "Epoch 480/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5750 - acc: 0.8065 - val_loss: 0.4557 - val_acc: 0.8554\n",
      "Epoch 481/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5759 - acc: 0.8089 - val_loss: 0.4515 - val_acc: 0.8570\n",
      "Epoch 482/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5724 - acc: 0.8066 - val_loss: 0.4561 - val_acc: 0.8551\n",
      "Epoch 483/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5668 - acc: 0.8098 - val_loss: 0.4559 - val_acc: 0.8561\n",
      "Epoch 484/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5728 - acc: 0.8087 - val_loss: 0.4560 - val_acc: 0.8559\n",
      "Epoch 485/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5691 - acc: 0.8079 - val_loss: 0.4549 - val_acc: 0.8560\n",
      "Epoch 486/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5681 - acc: 0.8104 - val_loss: 0.4526 - val_acc: 0.8583\n",
      "Epoch 487/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5740 - acc: 0.8085 - val_loss: 0.4573 - val_acc: 0.8550\n",
      "Epoch 488/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5745 - acc: 0.8061 - val_loss: 0.4628 - val_acc: 0.8542\n",
      "Epoch 489/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5748 - acc: 0.8078 - val_loss: 0.4570 - val_acc: 0.8551\n",
      "Epoch 490/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5753 - acc: 0.8052 - val_loss: 0.4598 - val_acc: 0.8543\n",
      "Epoch 491/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5727 - acc: 0.8083 - val_loss: 0.4588 - val_acc: 0.8553\n",
      "Epoch 492/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5737 - acc: 0.8058 - val_loss: 0.4547 - val_acc: 0.8561\n",
      "Epoch 493/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5720 - acc: 0.8083 - val_loss: 0.4579 - val_acc: 0.8557\n",
      "Epoch 494/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5684 - acc: 0.8095 - val_loss: 0.4600 - val_acc: 0.8541\n",
      "Epoch 495/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5739 - acc: 0.8064 - val_loss: 0.4576 - val_acc: 0.8550\n",
      "Epoch 496/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5715 - acc: 0.8069 - val_loss: 0.4604 - val_acc: 0.8544\n",
      "Epoch 497/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5761 - acc: 0.8104 - val_loss: 0.4605 - val_acc: 0.8550\n",
      "Epoch 498/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5696 - acc: 0.8104 - val_loss: 0.4531 - val_acc: 0.8560\n",
      "Epoch 499/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5727 - acc: 0.8083 - val_loss: 0.4573 - val_acc: 0.8542\n",
      "Epoch 500/500\n",
      "lrate:  1.220703125e-07\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5685 - acc: 0.8077 - val_loss: 0.4562 - val_acc: 0.8559\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(x_tr, y_tr, batch_size=32), steps_per_epoch=int(len(x_tr)/32), epochs=500, validation_data=(x_val, y_val), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "ZiQ3hcKxVrSj",
    "outputId": "d8e1ad40-8994-4211-ee2f-61eb55db4fc8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fW6P6Db7Vqh"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KazFarB57Vqi"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2K1zHog7Vqj"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#def reset_weights(model):\n",
    "#    session = K.get_session()\n",
    "#    for layer in model.layers: \n",
    "#        if hasattr(layer, 'kernel_initializer'):\n",
    "#           layer.kernel.initializer.run(session=session)\n",
    "\n",
    "#reset_weights(model)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.0),\n",
    "              metrics=['acc'])\n",
    "# learning schedule callback\n",
    "# lrate = LearningRateScheduler(step_decay())\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,  \n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True,\n",
    "        data_format='channels_last',\n",
    "        validation_split=0.0)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2BAA5U6G7Vqm",
    "outputId": "944bd854-7ad4-4dec-9d17-197f47fe2931",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 2.0755 - acc: 0.3105\n",
      "Epoch 2/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 1.5478 - acc: 0.4427\n",
      "Epoch 3/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 1.3934 - acc: 0.5017\n",
      "Epoch 4/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 1.2986 - acc: 0.5402 1s -\n",
      "Epoch 5/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 1.2247 - acc: 0.5698 0s - loss: 1.2256 - acc:\n",
      "Epoch 6/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 1.1693 - acc: 0.5878\n",
      "Epoch 7/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 1.1284 - acc: 0.6091\n",
      "Epoch 8/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 1.0873 - acc: 0.6210\n",
      "Epoch 9/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 1.0625 - acc: 0.6322 2s - loss: 1. - ETA: 1s - \n",
      "Epoch 10/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 1.0417 - acc: 0.6404 1s - loss: 1.0391 - acc: 0.641 - ETA:\n",
      "Epoch 11/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 1.0123 - acc: 0.6506 3s - loss: 1.01\n",
      "Epoch 12/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.9998 - acc: 0.6576\n",
      "Epoch 13/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.9758 - acc: 0.6644\n",
      "Epoch 14/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.9594 - acc: 0.6726\n",
      "Epoch 15/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.9500 - acc: 0.6738\n",
      "Epoch 16/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.9407 - acc: 0.6786\n",
      "Epoch 17/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.9276 - acc: 0.6834\n",
      "Epoch 18/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.9133 - acc: 0.6913 0s - loss: 0.9152 - \n",
      "Epoch 19/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.9028 - acc: 0.6913 4s - loss: 0.9 - ETA: 3s - \n",
      "Epoch 20/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.8999 - acc: 0.6952 1s - lo\n",
      "Epoch 21/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.8781 - acc: 0.7013\n",
      "Epoch 22/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.8708 - acc: 0.7042\n",
      "Epoch 23/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.8557 - acc: 0.7081\n",
      "Epoch 24/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.8528 - acc: 0.7124\n",
      "Epoch 25/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.8478 - acc: 0.7116\n",
      "Epoch 26/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.8319 - acc: 0.7198\n",
      "Epoch 27/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.8335 - acc: 0.7183\n",
      "Epoch 28/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.8285 - acc: 0.7174\n",
      "Epoch 29/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.8101 - acc: 0.7269\n",
      "Epoch 30/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.8181 - acc: 0.7215 0s - loss: 0.8186 - a\n",
      "Epoch 31/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.8081 - acc: 0.7285\n",
      "Epoch 32/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.8083 - acc: 0.7262\n",
      "Epoch 33/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.7990 - acc: 0.7298 10s - loss: 0.8032 - ac -  - ETA: 7s - loss: - ETA: 1s - loss: 0.7 - ETA: 0s - loss: 0.7990 - acc: 0.729\n",
      "Epoch 34/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.7934 - acc: 0.7322\n",
      "Epoch 35/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.7986 - acc: 0.7314\n",
      "Epoch 36/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.7863 - acc: 0.7334\n",
      "Epoch 37/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.7785 - acc: 0.7371\n",
      "Epoch 38/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.7763 - acc: 0.7389\n",
      "Epoch 39/500\n",
      "lrate:  0.0005\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.7683 - acc: 0.7402 0s - loss: 0.7684 - acc: \n",
      "Epoch 40/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.7422 - acc: 0.7496\n",
      "Epoch 41/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.7312 - acc: 0.7542 3s - \n",
      "Epoch 42/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.7289 - acc: 0.7545\n",
      "Epoch 43/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.7212 - acc: 0.7564\n",
      "Epoch 44/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.7201 - acc: 0.7587\n",
      "Epoch 45/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.7176 - acc: 0.7596 1s - lo\n",
      "Epoch 46/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.7094 - acc: 0.7613 1s - loss - ETA: 0s - loss: 0.7100 - acc: 0.7\n",
      "Epoch 47/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.7177 - acc: 0.7589\n",
      "Epoch 48/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.7160 - acc: 0.7572\n",
      "Epoch 49/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.7024 - acc: 0.7638ETA: 0s - loss: 0.7032 - acc: \n",
      "Epoch 50/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.7042 - acc: 0.7648\n",
      "Epoch 51/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.6998 - acc: 0.7631\n",
      "Epoch 52/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.7047 - acc: 0.7616 0s - loss: 0.7050 \n",
      "Epoch 53/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.7053 - acc: 0.7629\n",
      "Epoch 54/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.6970 - acc: 0.7647 0s - loss: 0.6973 - acc:\n",
      "Epoch 55/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6927 - acc: 0.7662\n",
      "Epoch 56/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6974 - acc: 0.7647\n",
      "Epoch 57/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6911 - acc: 0.7685\n",
      "Epoch 58/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.6903 - acc: 0.7666\n",
      "Epoch 59/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6913 - acc: 0.7679\n",
      "Epoch 60/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6891 - acc: 0.7658\n",
      "Epoch 61/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6877 - acc: 0.7675 0s - loss: 0.6881 - acc:\n",
      "Epoch 62/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6854 - acc: 0.7685\n",
      "Epoch 63/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6865 - acc: 0.7685\n",
      "Epoch 64/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6803 - acc: 0.7715\n",
      "Epoch 65/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6785 - acc: 0.7719\n",
      "Epoch 66/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6789 - acc: 0.7713\n",
      "Epoch 67/500\n",
      "lrate:  0.00025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6767 - acc: 0.7726\n",
      "Epoch 68/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6756 - acc: 0.7739\n",
      "Epoch 69/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6706 - acc: 0.7759 0s - loss: 0.6711\n",
      "Epoch 70/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6759 - acc: 0.7732\n",
      "Epoch 71/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6737 - acc: 0.7734\n",
      "Epoch 72/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.6688 - acc: 0.7742\n",
      "Epoch 73/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6701 - acc: 0.7764\n",
      "Epoch 74/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6750 - acc: 0.7729\n",
      "Epoch 75/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.6622 - acc: 0.7797 4\n",
      "Epoch 76/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6657 - acc: 0.7757\n",
      "Epoch 77/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6667 - acc: 0.7763 6s - - ETA: - ETA: 0s - loss: 0.6674 - acc\n",
      "Epoch 78/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.6631 - acc: 0.7774\n",
      "Epoch 79/500\n",
      "lrate:  0.00025\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6668 - acc: 0.7756\n",
      "Epoch 80/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6388 - acc: 0.7824\n",
      "Epoch 81/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.6480 - acc: 0.7829\n",
      "Epoch 82/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6442 - acc: 0.7833\n",
      "Epoch 83/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6381 - acc: 0.7863\n",
      "Epoch 84/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6400 - acc: 0.7862\n",
      "Epoch 85/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6454 - acc: 0.7843TA: 1s -\n",
      "Epoch 86/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.6419 - acc: 0.7845 0s - loss: 0.6412 \n",
      "Epoch 87/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.6284 - acc: 0.7878 1s - loss: \n",
      "Epoch 88/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6419 - acc: 0.7887\n",
      "Epoch 89/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6349 - acc: 0.7877\n",
      "Epoch 90/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6351 - acc: 0.7869\n",
      "Epoch 91/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6311 - acc: 0.7864\n",
      "Epoch 92/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6318 - acc: 0.7868\n",
      "Epoch 93/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.6355 - acc: 0.7856\n",
      "Epoch 94/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6256 - acc: 0.7890 2 - ETA: 0s - loss: 0.6256 - acc\n",
      "Epoch 95/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6343 - acc: 0.7866\n",
      "Epoch 96/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6224 - acc: 0.7918\n",
      "Epoch 97/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - ETA: 0s - loss: 0.6317 - acc: 0.7877- ETA: 0s - loss: 0.6320 - acc: 0.78 - 23s 14ms/step - loss: 0.6319 - acc: 0.7876\n",
      "Epoch 98/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6266 - acc: 0.7900\n",
      "Epoch 99/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6237 - acc: 0.7917\n",
      "Epoch 100/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6258 - acc: 0.7901\n",
      "Epoch 101/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6236 - acc: 0.7915\n",
      "Epoch 102/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6235 - acc: 0.7883 3s -  \n",
      "Epoch 103/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6264 - acc: 0.7908\n",
      "Epoch 104/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6213 - acc: 0.7909\n",
      "Epoch 105/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6289 - acc: 0.7896\n",
      "Epoch 106/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6270 - acc: 0.7893\n",
      "Epoch 107/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6257 - acc: 0.7910\n",
      "Epoch 108/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6231 - acc: 0.7904 0s - loss: 0.6232 - acc: 0.7\n",
      "Epoch 109/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6252 - acc: 0.7887\n",
      "Epoch 110/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6194 - acc: 0.7945\n",
      "Epoch 111/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6201 - acc: 0.7894 1s\n",
      "Epoch 112/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6189 - acc: 0.7927\n",
      "Epoch 113/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6176 - acc: 0.7916\n",
      "Epoch 114/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6223 - acc: 0.7900\n",
      "Epoch 115/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6126 - acc: 0.7933\n",
      "Epoch 116/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6200 - acc: 0.7921\n",
      "Epoch 117/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6193 - acc: 0.7914\n",
      "Epoch 118/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6100 - acc: 0.7972\n",
      "Epoch 119/500\n",
      "lrate:  0.000125\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6193 - acc: 0.7933\n",
      "Epoch 120/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6085 - acc: 0.7964 0s - loss: 0.608\n",
      "Epoch 121/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6051 - acc: 0.7979\n",
      "Epoch 122/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6061 - acc: 0.7969\n",
      "Epoch 123/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6064 - acc: 0.7988 1s - loss: 0.6040 - acc: 0.7 - ETA: 1s - los\n",
      "Epoch 124/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6077 - acc: 0.7963\n",
      "Epoch 125/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.6085 - acc: 0.7993\n",
      "Epoch 126/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6131 - acc: 0.7952\n",
      "Epoch 127/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6032 - acc: 0.7989\n",
      "Epoch 128/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6050 - acc: 0.7957 0s - loss: 0.6055 - acc: 0.\n",
      "Epoch 129/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.6014 - acc: 0.7978\n",
      "Epoch 130/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5977 - acc: 0.7997\n",
      "Epoch 131/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.6018 - acc: 0.7973\n",
      "Epoch 132/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.6066 - acc: 0.7956\n",
      "Epoch 133/500\n",
      "lrate:  6.25e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6025 - acc: 0.7963\n",
      "Epoch 134/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6004 - acc: 0.7994\n",
      "Epoch 135/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5979 - acc: 0.7992\n",
      "Epoch 136/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5993 - acc: 0.8004\n",
      "Epoch 137/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6031 - acc: 0.8003\n",
      "Epoch 138/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5961 - acc: 0.8005 0s - loss: 0.5959 - acc: 0.8\n",
      "Epoch 139/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6057 - acc: 0.7960 1s - l\n",
      "Epoch 140/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6042 - acc: 0.7971\n",
      "Epoch 141/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6010 - acc: 0.7977\n",
      "Epoch 142/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5942 - acc: 0.7994\n",
      "Epoch 143/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5976 - acc: 0.8003\n",
      "Epoch 144/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6037 - acc: 0.7964\n",
      "Epoch 145/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5987 - acc: 0.7991\n",
      "Epoch 146/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6006 - acc: 0.8006 1s - loss: 0.5965 - acc: 0.8 - ETA: 1s - lo\n",
      "Epoch 147/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5962 - acc: 0.8009\n",
      "Epoch 148/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5985 - acc: 0.7992\n",
      "Epoch 149/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5961 - acc: 0.8007 1s - lo\n",
      "Epoch 150/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6016 - acc: 0.7992\n",
      "Epoch 151/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.6040 - acc: 0.7970\n",
      "Epoch 152/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5880 - acc: 0.8025\n",
      "Epoch 153/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6031 - acc: 0.7979\n",
      "Epoch 154/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5924 - acc: 0.8005\n",
      "Epoch 155/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5915 - acc: 0.8030\n",
      "Epoch 156/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5995 - acc: 0.7995\n",
      "Epoch 157/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5949 - acc: 0.8006\n",
      "Epoch 158/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5977 - acc: 0.8004\n",
      "Epoch 159/500\n",
      "lrate:  6.25e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5983 - acc: 0.7988 0s - loss: 0.5996 - acc:\n",
      "Epoch 160/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5997 - acc: 0.8019\n",
      "Epoch 161/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5859 - acc: 0.8033\n",
      "Epoch 162/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5889 - acc: 0.8049\n",
      "Epoch 163/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5924 - acc: 0.7999\n",
      "Epoch 164/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5878 - acc: 0.8043\n",
      "Epoch 165/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.5913 - acc: 0.8012\n",
      "Epoch 166/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5831 - acc: 0.8018\n",
      "Epoch 167/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5909 - acc: 0.8020\n",
      "Epoch 168/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5865 - acc: 0.8039\n",
      "Epoch 169/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.5829 - acc: 0.8047\n",
      "Epoch 170/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5883 - acc: 0.8028\n",
      "Epoch 171/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5838 - acc: 0.8039\n",
      "Epoch 172/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5853 - acc: 0.8060\n",
      "Epoch 173/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5906 - acc: 0.8017\n",
      "Epoch 174/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5897 - acc: 0.8023\n",
      "Epoch 175/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5880 - acc: 0.8026\n",
      "Epoch 176/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5838 - acc: 0.8049 2s - loss: 0.5841 - acc:  - ETA: 1\n",
      "Epoch 177/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5872 - acc: 0.8016\n",
      "Epoch 178/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5856 - acc: 0.8037\n",
      "Epoch 179/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5839 - acc: 0.8049\n",
      "Epoch 180/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5864 - acc: 0.8028\n",
      "Epoch 181/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5929 - acc: 0.8036 0s - loss: 0.5940 - acc: 0.8\n",
      "Epoch 182/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5883 - acc: 0.8027 2s - loss: - ETA: 0s - loss: 0.58\n",
      "Epoch 183/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5832 - acc: 0.8040\n",
      "Epoch 184/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5925 - acc: 0.8026\n",
      "Epoch 185/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5798 - acc: 0.8069\n",
      "Epoch 186/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5818 - acc: 0.8037 1s - loss: 0\n",
      "Epoch 187/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5847 - acc: 0.8048\n",
      "Epoch 188/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5857 - acc: 0.8038  - ETA:  - ETA: 0s - loss: 0.5862 - acc: 0\n",
      "Epoch 189/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5859 - acc: 0.8023\n",
      "Epoch 190/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5836 - acc: 0.8049\n",
      "Epoch 191/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5766 - acc: 0.8074\n",
      "Epoch 192/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5835 - acc: 0.8058\n",
      "Epoch 193/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5842 - acc: 0.8047 0s - loss: 0.5842 - acc: 0.80\n",
      "Epoch 194/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5839 - acc: 0.8047\n",
      "Epoch 195/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.5837 - acc: 0.8058\n",
      "Epoch 196/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5882 - acc: 0.8023\n",
      "Epoch 197/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5850 - acc: 0.8027 6s - lo - \n",
      "Epoch 198/500\n",
      "lrate:  3.125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5904 - acc: 0.8033\n",
      "Epoch 199/500\n",
      "lrate:  3.125e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5814 - acc: 0.8054\n",
      "Epoch 200/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5780 - acc: 0.8060\n",
      "Epoch 201/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5826 - acc: 0.8058\n",
      "Epoch 202/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5848 - acc: 0.8034\n",
      "Epoch 203/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5817 - acc: 0.8032 0s - loss: 0.5812 - acc: 0\n",
      "Epoch 204/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5813 - acc: 0.8057\n",
      "Epoch 205/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5850 - acc: 0.8017\n",
      "Epoch 206/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5776 - acc: 0.8067\n",
      "Epoch 207/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5799 - acc: 0.8052\n",
      "Epoch 208/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5790 - acc: 0.8044 8 - ETA: 6\n",
      "Epoch 209/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5750 - acc: 0.8047\n",
      "Epoch 210/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5834 - acc: 0.8049\n",
      "Epoch 211/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5760 - acc: 0.8076\n",
      "Epoch 212/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5790 - acc: 0.8056\n",
      "Epoch 213/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5706 - acc: 0.8095\n",
      "Epoch 214/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5737 - acc: 0.8083\n",
      "Epoch 215/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5803 - acc: 0.8063 1s \n",
      "Epoch 216/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.5841 - acc: 0.8017\n",
      "Epoch 217/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5801 - acc: 0.8061 1s - loss\n",
      "Epoch 218/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5806 - acc: 0.8076\n",
      "Epoch 219/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5809 - acc: 0.8051\n",
      "Epoch 220/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5766 - acc: 0.8060 1s - loss: 0.5779  - ETA: 0s - loss: 0.57\n",
      "Epoch 221/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5826 - acc: 0.8034\n",
      "Epoch 222/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5772 - acc: 0.8066 0s - loss: 0.5784 - \n",
      "Epoch 223/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5781 - acc: 0.8059\n",
      "Epoch 224/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5826 - acc: 0.8059\n",
      "Epoch 225/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.5854 - acc: 0.8052\n",
      "Epoch 226/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5780 - acc: 0.8065\n",
      "Epoch 227/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5759 - acc: 0.8064\n",
      "Epoch 228/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5801 - acc: 0.8042\n",
      "Epoch 229/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5791 - acc: 0.8060\n",
      "Epoch 230/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.5798 - acc: 0.8073\n",
      "Epoch 231/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.5765 - acc: 0.8079 0s - loss: 0.5767 - acc:\n",
      "Epoch 232/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5750 - acc: 0.8074 1s\n",
      "Epoch 233/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5773 - acc: 0.8072 0s - loss: 0.5778 - acc: 0\n",
      "Epoch 234/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5758 - acc: 0.8069\n",
      "Epoch 235/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5824 - acc: 0.8055\n",
      "Epoch 236/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5809 - acc: 0.8056 1s - loss: 0.\n",
      "Epoch 237/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5774 - acc: 0.8074\n",
      "Epoch 238/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5691 - acc: 0.8088 0s - loss: 0.5697 - acc: 0\n",
      "Epoch 239/500\n",
      "lrate:  1.5625e-05\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5758 - acc: 0.8066\n",
      "Epoch 240/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5763 - acc: 0.8060 3s - loss: 0.5767 - acc: 0.805 - ET - ETA: 1s  - ETA: 0s - loss: 0.5766 - acc: 0.\n",
      "Epoch 241/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5821 - acc: 0.8064\n",
      "Epoch 242/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5769 - acc: 0.8053\n",
      "Epoch 243/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.5716 - acc: 0.8070\n",
      "Epoch 244/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5814 - acc: 0.8043\n",
      "Epoch 245/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5716 - acc: 0.8081\n",
      "Epoch 246/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5801 - acc: 0.8048\n",
      "Epoch 247/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5780 - acc: 0.8057\n",
      "Epoch 248/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5773 - acc: 0.8068 0s - loss: 0.5777 - ETA: 0s - loss: 0.5773 - acc: 0.806\n",
      "Epoch 249/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5780 - acc: 0.8090\n",
      "Epoch 250/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5761 - acc: 0.8067\n",
      "Epoch 251/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5756 - acc: 0.8058  - ET - ETA: 3s - loss: 0.5748 - acc:  - ETA: 3s - loss: 0 - E\n",
      "Epoch 252/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5727 - acc: 0.8091\n",
      "Epoch 253/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5773 - acc: 0.8064 \n",
      "Epoch 254/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5842 - acc: 0.8044\n",
      "Epoch 255/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5805 - acc: 0.8053\n",
      "Epoch 256/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5704 - acc: 0.8081\n",
      "Epoch 257/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5835 - acc: 0.8056\n",
      "Epoch 258/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5698 - acc: 0.8093\n",
      "Epoch 259/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5750 - acc: 0.8083\n",
      "Epoch 260/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5765 - acc: 0.8062\n",
      "Epoch 261/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5694 - acc: 0.8100\n",
      "Epoch 262/500\n",
      "lrate:  7.8125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5811 - acc: 0.8048\n",
      "Epoch 263/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5786 - acc: 0.8065\n",
      "Epoch 264/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.5712 - acc: 0.8082\n",
      "Epoch 265/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5743 - acc: 0.8076\n",
      "Epoch 266/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5804 - acc: 0.8055\n",
      "Epoch 267/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5757 - acc: 0.8089 0s - loss: 0.5757 - acc: 0.809\n",
      "Epoch 268/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5755 - acc: 0.8072\n",
      "Epoch 269/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5818 - acc: 0.8042\n",
      "Epoch 270/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5754 - acc: 0.8063\n",
      "Epoch 271/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5800 - acc: 0.8072\n",
      "Epoch 272/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5775 - acc: 0.8067 1s - loss: 0\n",
      "Epoch 273/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5656 - acc: 0.8100\n",
      "Epoch 274/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5801 - acc: 0.8070\n",
      "Epoch 275/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5790 - acc: 0.8058\n",
      "Epoch 276/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.5796 - acc: 0.8064\n",
      "Epoch 277/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5769 - acc: 0.8067 1s - \n",
      "Epoch 278/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5766 - acc: 0.8074 0s - loss: 0.5766 - ac\n",
      "Epoch 279/500\n",
      "lrate:  7.8125e-06\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.5783 - acc: 0.8057\n",
      "Epoch 280/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5742 - acc: 0.8065\n",
      "Epoch 281/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.5697 - acc: 0.8080 1s - los\n",
      "Epoch 282/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.5726 - acc: 0.8090\n",
      "Epoch 283/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5745 - acc: 0.8088\n",
      "Epoch 284/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5741 - acc: 0.8086\n",
      "Epoch 285/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5717 - acc: 0.8081ETA: 1s - lo\n",
      "Epoch 286/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5792 - acc: 0.8043\n",
      "Epoch 287/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5672 - acc: 0.8089\n",
      "Epoch 288/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5762 - acc: 0.8074\n",
      "Epoch 289/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5759 - acc: 0.8064\n",
      "Epoch 290/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5742 - acc: 0.8078 0s - loss: 0.5744 \n",
      "Epoch 291/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5774 - acc: 0.8076\n",
      "Epoch 292/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5790 - acc: 0.8063\n",
      "Epoch 293/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5698 - acc: 0.8101\n",
      "Epoch 294/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5801 - acc: 0.8047\n",
      "Epoch 295/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5754 - acc: 0.8070\n",
      "Epoch 296/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5701 - acc: 0.8104\n",
      "Epoch 297/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5773 - acc: 0.8066\n",
      "Epoch 298/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5739 - acc: 0.8083\n",
      "Epoch 299/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5704 - acc: 0.8103\n",
      "Epoch 300/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5677 - acc: 0.8094\n",
      "Epoch 301/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5733 - acc: 0.8082\n",
      "Epoch 302/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5744 - acc: 0.8088\n",
      "Epoch 303/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5760 - acc: 0.8072\n",
      "Epoch 304/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5711 - acc: 0.8095\n",
      "Epoch 305/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5686 - acc: 0.8080\n",
      "Epoch 306/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5722 - acc: 0.8086\n",
      "Epoch 307/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5798 - acc: 0.8083 1s - loss: 0.5800 - acc: 0.8 - ETA: 1s - lo\n",
      "Epoch 308/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5686 - acc: 0.8093\n",
      "Epoch 309/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5762 - acc: 0.8070\n",
      "Epoch 310/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5778 - acc: 0.8056 0s - loss: 0.5787 \n",
      "Epoch 311/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5714 - acc: 0.8108\n",
      "Epoch 312/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5730 - acc: 0.8084\n",
      "Epoch 313/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5762 - acc: 0.8068\n",
      "Epoch 314/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5691 - acc: 0.8096\n",
      "Epoch 315/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5772 - acc: 0.8068\n",
      "Epoch 316/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5804 - acc: 0.8046\n",
      "Epoch 317/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5746 - acc: 0.8074\n",
      "Epoch 318/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5707 - acc: 0.8094 0s - loss: 0.\n",
      "Epoch 319/500\n",
      "lrate:  3.90625e-06\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5770 - acc: 0.8068\n",
      "Epoch 320/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5696 - acc: 0.8108\n",
      "Epoch 321/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5746 - acc: 0.8087 0s - loss: 0.5742\n",
      "Epoch 322/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5701 - acc: 0.8108\n",
      "Epoch 323/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5652 - acc: 0.8115\n",
      "Epoch 324/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5738 - acc: 0.8061 \n",
      "Epoch 325/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5771 - acc: 0.8073\n",
      "Epoch 326/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5754 - acc: 0.8071\n",
      "Epoch 327/500\n",
      "lrate:  1.953125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5784 - acc: 0.8052\n",
      "Epoch 328/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5740 - acc: 0.8067\n",
      "Epoch 329/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5726 - acc: 0.8072\n",
      "Epoch 330/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5710 - acc: 0.8091\n",
      "Epoch 331/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5679 - acc: 0.8100\n",
      "Epoch 332/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5737 - acc: 0.8062\n",
      "Epoch 333/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5757 - acc: 0.8066\n",
      "Epoch 334/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5751 - acc: 0.8068\n",
      "Epoch 335/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5749 - acc: 0.8083 0s - loss: 0.5748 - acc: \n",
      "Epoch 336/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5685 - acc: 0.8092\n",
      "Epoch 337/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5726 - acc: 0.8095\n",
      "Epoch 338/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5688 - acc: 0.8114\n",
      "Epoch 339/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5696 - acc: 0.8090\n",
      "Epoch 340/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5721 - acc: 0.8090\n",
      "Epoch 341/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5805 - acc: 0.8067 0s - loss: 0.5804 - acc:\n",
      "Epoch 342/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5670 - acc: 0.8092 3s - l - ETA: \n",
      "Epoch 343/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5722 - acc: 0.8091\n",
      "Epoch 344/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5634 - acc: 0.8104\n",
      "Epoch 345/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5716 - acc: 0.8061 0s - loss: 0.5712 - \n",
      "Epoch 346/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5815 - acc: 0.8068\n",
      "Epoch 347/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5752 - acc: 0.8063\n",
      "Epoch 348/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5687 - acc: 0.8109\n",
      "Epoch 349/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5739 - acc: 0.8076\n",
      "Epoch 350/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5722 - acc: 0.8084\n",
      "Epoch 351/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5726 - acc: 0.8092 1s -\n",
      "Epoch 352/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5716 - acc: 0.8099\n",
      "Epoch 353/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5711 - acc: 0.8096\n",
      "Epoch 354/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5680 - acc: 0.8102 - ETA: 3s -  - ETA: 1s - l\n",
      "Epoch 355/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - ETA: 0s - loss: 0.5728 - acc: 0.808 - 22s 14ms/step - loss: 0.5731 - acc: 0.8084\n",
      "Epoch 356/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5714 - acc: 0.8085\n",
      "Epoch 357/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5675 - acc: 0.8116 3s - loss: 0.5681 \n",
      "Epoch 358/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5709 - acc: 0.8080\n",
      "Epoch 359/500\n",
      "lrate:  1.953125e-06\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5744 - acc: 0.8071\n",
      "Epoch 360/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5750 - acc: 0.8052ETA: 0s - loss: 0.5744 - a\n",
      "Epoch 361/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5699 - acc: 0.8094\n",
      "Epoch 362/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5680 - acc: 0.8076\n",
      "Epoch 363/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5719 - acc: 0.8081\n",
      "Epoch 364/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5769 - acc: 0.8050\n",
      "Epoch 365/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5764 - acc: 0.8069\n",
      "Epoch 366/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5669 - acc: 0.8086\n",
      "Epoch 367/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5745 - acc: 0.8082\n",
      "Epoch 368/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5741 - acc: 0.8081\n",
      "Epoch 369/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5698 - acc: 0.8095\n",
      "Epoch 370/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5735 - acc: 0.8084\n",
      "Epoch 371/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5745 - acc: 0.8075\n",
      "Epoch 372/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5739 - acc: 0.8064\n",
      "Epoch 373/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5735 - acc: 0.8050\n",
      "Epoch 374/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.5692 - acc: 0.8079\n",
      "Epoch 375/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5749 - acc: 0.8072\n",
      "Epoch 376/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5759 - acc: 0.8065\n",
      "Epoch 377/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - ETA: 0s - loss: 0.5736 - acc: 0.807 - 22s 14ms/step - loss: 0.5735 - acc: 0.8074\n",
      "Epoch 378/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 3014s 2s/step - loss: 0.5728 - acc: 0.8062\n",
      "Epoch 379/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5766 - acc: 0.8082\n",
      "Epoch 380/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5708 - acc: 0.8085\n",
      "Epoch 381/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5735 - acc: 0.8071\n",
      "Epoch 382/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5773 - acc: 0.8067\n",
      "Epoch 383/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5761 - acc: 0.8066\n",
      "Epoch 384/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5761 - acc: 0.8064\n",
      "Epoch 385/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5669 - acc: 0.8100\n",
      "Epoch 386/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5797 - acc: 0.8054\n",
      "Epoch 387/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5728 - acc: 0.8072\n",
      "Epoch 388/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5730 - acc: 0.8076\n",
      "Epoch 389/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5707 - acc: 0.8103\n",
      "Epoch 390/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5746 - acc: 0.8074\n",
      "Epoch 391/500\n",
      "lrate:  9.765625e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5743 - acc: 0.8105\n",
      "Epoch 392/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - ETA: 0s - loss: 0.5779 - acc: 0.8061- ETA: 0s - loss: 0.5779 - acc:  - 23s 15ms/step - loss: 0.5780 - acc: 0.8061\n",
      "Epoch 393/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5765 - acc: 0.8062\n",
      "Epoch 394/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5630 - acc: 0.8108\n",
      "Epoch 395/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5775 - acc: 0.8069\n",
      "Epoch 396/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.5748 - acc: 0.8065\n",
      "Epoch 397/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5702 - acc: 0.8077\n",
      "Epoch 398/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5730 - acc: 0.8061\n",
      "Epoch 399/500\n",
      "lrate:  9.765625e-07\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5759 - acc: 0.8082 1s - loss: 0.\n",
      "Epoch 400/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 34s 22ms/step - loss: 0.5741 - acc: 0.8083\n",
      "Epoch 401/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 42s 27ms/step - loss: 0.5754 - acc: 0.8081\n",
      "Epoch 402/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5752 - acc: 0.8064\n",
      "Epoch 403/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5750 - acc: 0.8074\n",
      "Epoch 404/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5706 - acc: 0.8079 1s - loss: 0.5680 - acc: 0 - ETA: 1s - loss: 0\n",
      "Epoch 405/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5717 - acc: 0.8103\n",
      "Epoch 406/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5731 - acc: 0.8090\n",
      "Epoch 407/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5750 - acc: 0.8069\n",
      "Epoch 408/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5777 - acc: 0.8058\n",
      "Epoch 409/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5701 - acc: 0.8092 0s - loss: 0.5702 - acc: 0.809\n",
      "Epoch 410/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5749 - acc: 0.8064\n",
      "Epoch 411/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5674 - acc: 0.8077\n",
      "Epoch 412/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5709 - acc: 0.8068\n",
      "Epoch 413/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5729 - acc: 0.8079\n",
      "Epoch 414/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5713 - acc: 0.8100 2s - loss: 0.5726 - ac - ETA\n",
      "Epoch 415/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5744 - acc: 0.8068\n",
      "Epoch 416/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5667 - acc: 0.8090 2s - loss: 0. - ETA:\n",
      "Epoch 417/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5703 - acc: 0.8077\n",
      "Epoch 418/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5664 - acc: 0.8097\n",
      "Epoch 419/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5714 - acc: 0.8067\n",
      "Epoch 420/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5700 - acc: 0.8092\n",
      "Epoch 421/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5776 - acc: 0.8062 0s - loss: 0.5759 -\n",
      "Epoch 422/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5638 - acc: 0.8097\n",
      "Epoch 423/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5748 - acc: 0.8072 1s - loss:\n",
      "Epoch 424/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5664 - acc: 0.8093\n",
      "Epoch 425/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5715 - acc: 0.8078\n",
      "Epoch 426/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5708 - acc: 0.8092 4s - loss: 0.5722 -  - ETA\n",
      "Epoch 427/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5727 - acc: 0.8084\n",
      "Epoch 428/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5750 - acc: 0.8046\n",
      "Epoch 429/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5741 - acc: 0.8062\n",
      "Epoch 430/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5729 - acc: 0.8076\n",
      "Epoch 431/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5709 - acc: 0.8092\n",
      "Epoch 432/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5706 - acc: 0.8094A: 2s - loss: 0.5718  - ETA: 1\n",
      "Epoch 433/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5739 - acc: 0.8087\n",
      "Epoch 434/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5725 - acc: 0.8071 0s - loss: 0.5\n",
      "Epoch 435/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5756 - acc: 0.8066\n",
      "Epoch 436/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5639 - acc: 0.8112\n",
      "Epoch 437/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5703 - acc: 0.8106\n",
      "Epoch 438/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5674 - acc: 0.8097\n",
      "Epoch 439/500\n",
      "lrate:  4.8828125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5724 - acc: 0.8068\n",
      "Epoch 440/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5751 - acc: 0.8058\n",
      "Epoch 441/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5702 - acc: 0.8096\n",
      "Epoch 442/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5776 - acc: 0.8075\n",
      "Epoch 443/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5722 - acc: 0.8081\n",
      "Epoch 444/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5686 - acc: 0.8082 1s - l\n",
      "Epoch 445/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5761 - acc: 0.8080\n",
      "Epoch 446/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5693 - acc: 0.8095\n",
      "Epoch 447/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5720 - acc: 0.8085 5s - loss: 0.5742 - acc:  - ETA: 5s - loss: 0.5741 - a - ETA: 4s - loss: 0.5741 - a - ETA: 3\n",
      "Epoch 448/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5789 - acc: 0.8075\n",
      "Epoch 449/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 21s 13ms/step - loss: 0.5701 - acc: 0.8091\n",
      "Epoch 450/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5731 - acc: 0.8079 3s - l\n",
      "Epoch 451/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5759 - acc: 0.8081\n",
      "Epoch 452/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 23s 14ms/step - loss: 0.5686 - acc: 0.8087\n",
      "Epoch 453/500\n",
      "lrate:  2.44140625e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5725 - acc: 0.8075\n",
      "Epoch 454/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5730 - acc: 0.8104\n",
      "Epoch 455/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5714 - acc: 0.8085\n",
      "Epoch 456/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5734 - acc: 0.8064 0s - loss: 0.5734 - acc\n",
      "Epoch 457/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5721 - acc: 0.8091\n",
      "Epoch 458/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5776 - acc: 0.8065 0s - loss: 0.5769 -\n",
      "Epoch 459/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5711 - acc: 0.8097\n",
      "Epoch 460/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5720 - acc: 0.8067 0s - loss: 0.5718 - a\n",
      "Epoch 461/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5764 - acc: 0.8066\n",
      "Epoch 462/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5718 - acc: 0.8092\n",
      "Epoch 463/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5706 - acc: 0.8091\n",
      "Epoch 464/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5747 - acc: 0.8072 0s - loss: 0.5757 - acc: 0\n",
      "Epoch 465/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5751 - acc: 0.8070\n",
      "Epoch 466/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5670 - acc: 0.8104\n",
      "Epoch 467/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5785 - acc: 0.8069\n",
      "Epoch 468/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5692 - acc: 0.8076\n",
      "Epoch 469/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5642 - acc: 0.8100\n",
      "Epoch 470/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5739 - acc: 0.8082\n",
      "Epoch 471/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5714 - acc: 0.8088\n",
      "Epoch 472/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5711 - acc: 0.8082\n",
      "Epoch 473/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5752 - acc: 0.8090 2s - - ETA: 1s - loss: 0.\n",
      "Epoch 474/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5740 - acc: 0.8068\n",
      "Epoch 475/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5774 - acc: 0.8079\n",
      "Epoch 476/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5696 - acc: 0.8110\n",
      "Epoch 477/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5742 - acc: 0.8066\n",
      "Epoch 478/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5637 - acc: 0.8120\n",
      "Epoch 479/500\n",
      "lrate:  2.44140625e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5790 - acc: 0.8055 4s - loss: 0.5 - ETA: 3s - loss: 0\n",
      "Epoch 480/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - ETA: 0s - loss: 0.5697 - acc: 0.808 - 26s 17ms/step - loss: 0.5698 - acc: 0.8082\n",
      "Epoch 481/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5727 - acc: 0.8096\n",
      "Epoch 482/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5655 - acc: 0.8099 2s - loss: 0.5657 - ac - ETA: 1s - l\n",
      "Epoch 483/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5778 - acc: 0.8056\n",
      "Epoch 484/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 24s 16ms/step - loss: 0.5812 - acc: 0.8062\n",
      "Epoch 485/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5754 - acc: 0.8056: 0s - loss: 0.5748 - acc: \n",
      "Epoch 486/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5743 - acc: 0.8067\n",
      "Epoch 487/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5736 - acc: 0.8069\n",
      "Epoch 488/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5704 - acc: 0.8092\n",
      "Epoch 489/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5785 - acc: 0.8063 0s - loss: 0.5782 - acc: \n",
      "Epoch 490/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 24s 15ms/step - loss: 0.5672 - acc: 0.8096 1s - loss: 0.5\n",
      "Epoch 491/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 23s 15ms/step - loss: 0.5699 - acc: 0.8083\n",
      "Epoch 492/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5673 - acc: 0.8104\n",
      "Epoch 493/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5685 - acc: 0.8085\n",
      "Epoch 494/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5701 - acc: 0.8095: 0s - loss: 0.5689\n",
      "Epoch 495/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5691 - acc: 0.8089 0s - loss: 0.5688 - acc: 0\n",
      "Epoch 496/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5749 - acc: 0.8088\n",
      "Epoch 497/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5737 - acc: 0.8074 0s - loss: 0.57\n",
      "Epoch 498/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5792 - acc: 0.8080\n",
      "Epoch 499/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5729 - acc: 0.8096\n",
      "Epoch 500/500\n",
      "lrate:  1.220703125e-07\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.5703 - acc: 0.8097\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(x_train, y_train_vec, batch_size=32), steps_per_epoch=int(len(x_train)/32), epochs=500, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lZKZN0X7Vqp"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_Qv_N9J7Vqq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 238us/step\n",
      "loss = 0.46797399830818176\n",
      "accuracy = 0.8555999994277954\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsrIRtpw7Vqt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of HM4-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
